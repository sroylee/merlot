{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Similarity Feature: Similary between query item and user's activities\n",
    "Given a query item $q$ (i.e., a Stack Overflow question or GitHub repository) and a user $u$, we compute averange similarity between $q$ and all items where $u$ has perform an activity $a$ on. For example, given an item question $q_1$, we compute the average similarity $q_1$ and all other questions where beween user $u_1$ has answered. I.e., in this case, the activity $a$ will be the *answer* activity. The similarity function is given as below: \n",
    "\n",
    "$Sim(u,q,a) = \\frac{|\\{i\\in I_{(u,a)}|i_{tags}\\in q_{tags}\\}|}{|I_{(u,a)}|}$\n",
    "\n",
    "Where, $<u,q,a>$ is a triplet of a user $u$, query item $q$ and specific activity $a$. We say that a query item $q$ is similar to a user $u$'s $a$ activities, when many items, which $u$ performed $a$ on, shares similar tags with $q$. The above similarity function captures this intuition. The numerator computes the number of items where $u$ perform $a$ and the item shares at least 1 tag with the query item $q$. The denominator computes the total number of items where $u$ perform $a$\n",
    "\n",
    "Notations\n",
    "- $u$: User\n",
    "- $a$: Activity. Different activities performed by user. E.g. answer, favorite, fork and watch.\n",
    "- $i$: Item, i.e., Stack Overflow question or GitHub repository\n",
    "- $q$: Query item\n",
    "- $I_{(u,a)}$: Items where user $u$ perform activity $a$ on. E.g. questions which are answered by a user. \n",
    "- $i_{tags}$: Tags for the item. E.g. *Java*, *iOS*, etc. \n",
    "- $q_{tags}$: Tags for query item.\n",
    "\n",
    "For function UserActivitySim():\n",
    "- q: query item id (str)\n",
    "- I_ua: ids of items where user has perform an activity (list)\n",
    "- I_tags: key is item id and value is tags (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UserActivitySim(q, I_ua, I_tags):\n",
    "    if q in I_ua:\n",
    "        I_ua.remove(q)\n",
    "    if len(I_ua)==0:\n",
    "        return 0\n",
    "    q_tags = set(I_tags[q].split(' '))\n",
    "    numerator = 0\n",
    "    for i in I_ua:\n",
    "        i_tags = set(I_tags[i].split(' '))\n",
    "        overlap = i_tags.intersection(q_tags)\n",
    "        if len(overlap) >0:\n",
    "            numerator += 1\n",
    "    return numerator/len(I_ua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We use [Mean Average Precision (MAP)](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) to evaluate the activity prediciton tasks. I.e., we take total average precision (AP) of each user divided by the total number of users. \n",
    "\n",
    "MAP function:\n",
    "- groundtruth: a dictionary where key is user id and value is the test labels for all the positive/negative instance for this use\n",
    "- pred: a dictionary where key is the user id and value is the predicted probabilities of the label being posititive (i.e,'1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAP(groundtruth,pred):\n",
    "    result = 0\n",
    "    for key, value in groundtruth.items():\n",
    "        y_truth = value\n",
    "        y_pred = pred[key]\n",
    "        score= average_precision_score(y_truth,y_pred)\n",
    "        result +=score\n",
    "    return result/len(groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Activity Prediction \n",
    "#### Setup 1: Using only Answer Similarity Feature\n",
    "- Compute the similarity scores for answer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'toy_data/new_format/training/user_answer_training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1d6168e62b5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mans_user_items_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mans_item_tags_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_training_activities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'toy_data/new_format/training/user_answer_training.csv'"
     ]
    }
   ],
   "source": [
    "i_users = 'toy_data/users.csv'\n",
    "i_training_activities = 'toy_data/new_format/training/user_answer_training.csv'\n",
    "i_testing_activities = 'toy_data/new_format/test/user_answer_testing.csv'\n",
    "\n",
    "#Load answer training set\n",
    "ans_u_ids_train = []\n",
    "ans_q_ids_train = []\n",
    "ans_q_label_train = []\n",
    "ans_user_items_train = {} \n",
    "ans_item_tags_train = {}\n",
    "with open(i_training_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        ans_u_ids_train.append(uid)\n",
    "        ans_q_ids_train.append(rid)\n",
    "        ans_q_label_train.append(label)\n",
    "        if label==1:\n",
    "            if uid in ans_user_items_train:\n",
    "                ans_user_items_train[uid].append(rid)\n",
    "            else:\n",
    "                ans_user_items_train[uid] = [rid]\n",
    "        ans_item_tags_train[rid] = tags\n",
    "\n",
    "#Compute score for answer training set\n",
    "ans_scores_train = []\n",
    "for i in range(len(ans_u_ids_train)):\n",
    "    q = ans_q_ids_train[i]\n",
    "    uid = ans_u_ids_train[i]\n",
    "    I_ua = ans_user_items_train[uid].copy()\n",
    "    I_tags = ans_item_tags_train\n",
    "    score = UserActivitySim(q, I_ua, I_tags)\n",
    "    ans_scores_train.append(score)\n",
    "    \n",
    "#Load answer test set\n",
    "ans_u_ids_test = []\n",
    "ans_q_ids_test = []\n",
    "ans_q_label_test = []\n",
    "ans_user_items_test = {} \n",
    "ans_item_tags_test = {}\n",
    "with open(i_testing_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        ans_u_ids_test.append(uid)\n",
    "        ans_q_ids_test.append(rid)\n",
    "        ans_q_label_test.append(label)\n",
    "        if label==1:\n",
    "            if uid in ans_user_items_test:\n",
    "                ans_user_items_test[uid].append(rid)\n",
    "            else:\n",
    "                ans_user_items_test[uid] = [rid]\n",
    "        ans_item_tags_test[rid] = tags\n",
    "\n",
    "#Compute score for answer test set\n",
    "ans_scores_test = []\n",
    "for i in range(len(ans_u_ids_test)):\n",
    "    q = ans_q_ids_test[i]\n",
    "    uid = ans_u_ids_test[i]\n",
    "    I_ua = ans_user_items_test[uid].copy()\n",
    "    I_tags = ans_item_tags_test\n",
    "\n",
    "    score = UserActivitySim(q, I_ua, I_tags)\n",
    "    ans_scores_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "x_train_list = []\n",
    "for i in range(len(ans_scores_train)):\n",
    "    x_train_list.append([int(ans_u_ids_train[i]),int(ans_scores_train[i])])\n",
    "x_train = np.array(x_train_list)\n",
    "y_train = ans_q_label_train.copy()\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "#clf = svm.SVC(kernel='linear')\n",
    "clf.fit(x_train_list,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "x_test_list = []\n",
    "for i in range(len(ans_scores_test)):\n",
    "    x_test_list.append([int(ans_u_ids_test[i]),int(ans_scores_test[i])])\n",
    "x_test = np.array(x_test_list)\n",
    "y_test = ans_q_label_test.copy()\n",
    "#pred = clf.predict(x_test)\n",
    "pred = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "user_level_truth = {}\n",
    "user_level_pred = {}\n",
    "for i in range(len(pred)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_pred:\n",
    "        user_level_pred[uid] = [pred[i][0]]\n",
    "    else:\n",
    "        user_level_pred[uid].append(pred[i][1])\n",
    "\n",
    "for i in range(len(ans_q_label_test)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_truth:\n",
    "        user_level_truth[uid] = [ans_q_label_test[i]]\n",
    "    else:\n",
    "        user_level_truth[uid].append(ans_q_label_test[i])\n",
    "\n",
    "result = MAP(user_level_truth,user_level_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$error = \\sum_{i,j}(V_{i}^{T}V_{j} - S^{item}_{i,j})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
