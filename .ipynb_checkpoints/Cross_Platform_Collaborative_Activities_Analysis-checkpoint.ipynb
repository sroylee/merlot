{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Similarity Feature: Similary between query item and user's activities\n",
    "Given a query item $q$ (i.e., a Stack Overflow question or GitHub repository) and a user $u$, we compute averange similarity between $q$ and all items where $u$ has perform an activity $a$ on. For example, given an item question $q_1$, we compute the average similarity $q_1$ and all other questions where beween user $u_1$ has answered. I.e., in this case, the activity $a$ will be the *answer* activity. The similarity function is given as below: \n",
    "\n",
    "$Sim(u,q,a) = \\frac{|\\{i\\in I_{(u,a)}|i_{tags}\\in q_{tags}\\}|}{|I_{(u,a)}|}$\n",
    "\n",
    "Where, $<u,q,a>$ is a triplet of a user $u$, query item $q$ and specific activity $a$. We say that a query item $q$ is similar to a user $u$'s $a$ activities, when many items, which $u$ performed $a$ on, shares similar tags with $q$. The above similarity function captures this intuition. The numerator computes the number of items where $u$ perform $a$ and the item shares at least 1 tag with the query item $q$. The denominator computes the total number of items where $u$ perform $a$\n",
    "\n",
    "Notations\n",
    "- $u$: User\n",
    "- $a$: Activity. Different activities performed by user. E.g. answer, favorite, fork and watch.\n",
    "- $i$: Item, i.e., Stack Overflow question or GitHub repository\n",
    "- $q$: Query item\n",
    "- $I_{(u,a)}$: Items where user $u$ perform activity $a$ on. E.g. questions which are answered by a user. \n",
    "- $i_{tags}$: Tags for the item. E.g. *Java*, *iOS*, etc. \n",
    "- $q_{tags}$: Tags for query item.\n",
    "\n",
    "For function UserActivitySim():\n",
    "- q: query item id (str)\n",
    "- I_ua: ids of items where user has perform an activity (list)\n",
    "- I_tags: key is item id and value is tags (dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UserActivitySim(q, I_ua, Q_tags, I_tags):\n",
    "    if q in I_ua:\n",
    "        I_ua.remove(q)\n",
    "    if len(I_ua)==0:\n",
    "        return 0\n",
    "    q_tags = set(Q_tags[q].split(' '))\n",
    "    numerator = 0\n",
    "    for i in I_ua:\n",
    "        i_tags = set(I_tags[i].split(' '))\n",
    "        overlap = i_tags.intersection(q_tags)\n",
    "        if len(overlap) >0:\n",
    "            numerator += 1\n",
    "    return numerator/len(I_ua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric\n",
    "We use [Mean Average Precision (MAP)](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) to evaluate the activity prediciton tasks. I.e., we take total average precision (AP) of each user divided by the total number of users. \n",
    "\n",
    "MAP function:\n",
    "- groundtruth: a dictionary where key is user id and value is the test labels for all the positive/negative instance for this use\n",
    "- pred: a dictionary where key is the user id and value is the predicted probabilities of the label being posititive (i.e,'1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAP(groundtruth,pred):\n",
    "    result = 0\n",
    "    for key, value in groundtruth.items():\n",
    "        y_truth = value\n",
    "        y_pred = pred[key]\n",
    "        score= average_precision_score(y_truth,y_pred)\n",
    "        result +=score\n",
    "    return result/len(groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training and Test Data \n",
    "- i_users: user file\n",
    "- i_ans_training_activities: user's answer activities in Stack Overflow use for training\n",
    "- i_ans_testing_activities: user's answer activities in Stack Overflow use for test\n",
    "- i_fav_training_activities: user's favorite activities in Stack Overflow use for training\n",
    "- i_fav_testing_activities: user's favorite activities in Stack Overflow use for test\n",
    "- i_watch_training_activities: user's watch activities in GitHub use for training\n",
    "- i_watch_testing_activities: user's watch activities in GitHub use for test\n",
    "- i_fork_training_activities: user's fork activities in GitHub use for training\n",
    "- i_fork_testing_activities: user's fork activities in GitHub use for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_users = 'toy_data/users.csv'\n",
    "i_ans_training_activities = 'toy_data/training/user_answer_training.csv'\n",
    "i_ans_testing_activities = 'toy_data/test/user_answer_testing.csv'\n",
    "i_fav_training_activities = 'toy_data/training/user_favorite_training.csv'\n",
    "i_fav_testing_activities = 'toy_data/test/user_favorite_testing.csv'\n",
    "i_watch_training_activities = 'toy_data/training/user_watch_training.csv'\n",
    "i_watch_testing_activities = 'toy_data/test/user_watch_testing.csv'\n",
    "i_fork_training_activities = 'toy_data/training/user_fork_training.csv'\n",
    "i_fork_testing_activities = 'toy_data/test/user_fork_testing.csv'\n",
    "\n",
    "#Load answer training set\n",
    "ans_u_ids_train = []\n",
    "ans_q_ids_train = []\n",
    "ans_q_label_train = []\n",
    "ans_user_items_train = {} \n",
    "ans_item_tags_train = {}\n",
    "with open(i_ans_training_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        ans_u_ids_train.append(uid)\n",
    "        ans_q_ids_train.append(rid)\n",
    "        ans_q_label_train.append(label)\n",
    "        if label==1:\n",
    "            if uid in ans_user_items_train:\n",
    "                ans_user_items_train[uid].append(rid)\n",
    "            else:\n",
    "                ans_user_items_train[uid] = [rid]\n",
    "        ans_item_tags_train[rid] = tags\n",
    "    \n",
    "#Load answer test set\n",
    "ans_u_ids_test = []\n",
    "ans_q_ids_test = []\n",
    "ans_q_label_test = []\n",
    "ans_user_items_test = {} \n",
    "ans_item_tags_test = {}\n",
    "with open(i_ans_testing_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        ans_u_ids_test.append(uid)\n",
    "        ans_q_ids_test.append(rid)\n",
    "        ans_q_label_test.append(label)\n",
    "        if label==1:\n",
    "            if uid in ans_user_items_test:\n",
    "                ans_user_items_test[uid].append(rid)\n",
    "            else:\n",
    "                ans_user_items_test[uid] = [rid]\n",
    "        ans_item_tags_test[rid] = tags\n",
    "\n",
    "#Load favorite training set\n",
    "fav_u_ids_train = []\n",
    "fav_q_ids_train = []\n",
    "fav_q_label_train = []\n",
    "fav_user_items_train = {} \n",
    "fav_item_tags_train = {}\n",
    "with open(i_fav_training_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        fav_u_ids_train.append(uid)\n",
    "        fav_q_ids_train.append(rid)\n",
    "        fav_q_label_train.append(label)\n",
    "        if label==1:\n",
    "            if uid in fav_user_items_train:\n",
    "                fav_user_items_train[uid].append(rid)\n",
    "            else:\n",
    "                fav_user_items_train[uid] = [rid]\n",
    "        fav_item_tags_train[rid] = tags\n",
    "        \n",
    "#Load favorite test set\n",
    "fav_u_ids_test = []\n",
    "fav_q_ids_test = []\n",
    "fav_q_label_test = []\n",
    "fav_user_items_test = {} \n",
    "fav_item_tags_test = {}\n",
    "with open(i_fav_testing_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        fav_u_ids_test.append(uid)\n",
    "        fav_q_ids_test.append(rid)\n",
    "        fav_q_label_test.append(label)\n",
    "        if label==1:\n",
    "            if uid in fav_user_items_test:\n",
    "                fav_user_items_test[uid].append(rid)\n",
    "            else:\n",
    "                fav_user_items_test[uid] = [rid]\n",
    "        fav_item_tags_test[rid] = tags\n",
    "\n",
    "#Load watch training set\n",
    "watch_u_ids_train = []\n",
    "watch_q_ids_train = []\n",
    "watch_q_label_train = []\n",
    "watch_user_items_train = {} \n",
    "watch_item_tags_train = {}\n",
    "with open(i_watch_training_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        watch_u_ids_train.append(uid)\n",
    "        watch_q_ids_train.append(rid)\n",
    "        watch_q_label_train.append(label)\n",
    "        if label==1:\n",
    "            if uid in watch_user_items_train:\n",
    "                watch_user_items_train[uid].append(rid)\n",
    "            else:\n",
    "                watch_user_items_train[uid] = [rid]\n",
    "        watch_item_tags_train[rid] = tags\n",
    "        \n",
    "#Load watch test set\n",
    "watch_u_ids_test = []\n",
    "watch_q_ids_test = []\n",
    "watch_q_label_test = []\n",
    "watch_user_items_test = {} \n",
    "watch_item_tags_test = {}\n",
    "with open(i_watch_testing_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        watch_u_ids_test.append(uid)\n",
    "        watch_q_ids_test.append(rid)\n",
    "        watch_q_label_test.append(label)\n",
    "        if label==1:\n",
    "            if uid in watch_user_items_test:\n",
    "                watch_user_items_test[uid].append(rid)\n",
    "            else:\n",
    "                watch_user_items_test[uid] = [rid]\n",
    "        watch_item_tags_test[rid] = tags\n",
    "\n",
    "#Load fork training set\n",
    "fork_u_ids_train = []\n",
    "fork_q_ids_train = []\n",
    "fork_q_label_train = []\n",
    "fork_user_items_train = {} \n",
    "fork_item_tags_train = {}\n",
    "with open(i_fork_training_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        fork_u_ids_train.append(uid)\n",
    "        fork_q_ids_train.append(rid)\n",
    "        fork_q_label_train.append(label)\n",
    "        if label==1:\n",
    "            if uid in fork_user_items_train:\n",
    "                fork_user_items_train[uid].append(rid)\n",
    "            else:\n",
    "                fork_user_items_train[uid] = [rid]\n",
    "        fork_item_tags_train[rid] = tags\n",
    "        \n",
    "#Load fork test set\n",
    "fork_u_ids_test = []\n",
    "fork_q_ids_test = []\n",
    "fork_q_label_test = []\n",
    "fork_user_items_test = {} \n",
    "fork_item_tags_test = {}\n",
    "with open(i_fork_testing_activities, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        uid = int(row[0])\n",
    "        rid = str(row[1])\n",
    "        tags = str(row[2])\n",
    "        label = int(row[3])\n",
    "        fork_u_ids_test.append(uid)\n",
    "        fork_q_ids_test.append(rid)\n",
    "        fork_q_label_test.append(label)\n",
    "        if label==1:\n",
    "            if uid in fork_user_items_test:\n",
    "                fork_user_items_test[uid].append(rid)\n",
    "            else:\n",
    "                fork_user_items_test[uid] = [rid]\n",
    "        fork_item_tags_test[rid] = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Activity Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Activity Similarity Features for Answer Activity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute score for answer training set using user's answer activities\n",
    "ans_scores_train = []\n",
    "for i in range(len(ans_u_ids_train)):\n",
    "    q = ans_q_ids_train[i]\n",
    "    uid = ans_u_ids_train[i]\n",
    "    I_ua = ans_user_items_train[uid].copy()\n",
    "    Q_tags = ans_item_tags_train\n",
    "    I_tags = ans_item_tags_train\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    ans_scores_train.append(score)\n",
    "\n",
    "#Compute score for answer test set using user's answer activities\n",
    "ans_scores_test = []\n",
    "for i in range(len(ans_u_ids_test)):\n",
    "    q = ans_q_ids_test[i]\n",
    "    uid = ans_u_ids_test[i]\n",
    "    I_ua = ans_user_items_test[uid].copy()\n",
    "    Q_tags = ans_item_tags_test\n",
    "    I_tags = ans_item_tags_test\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    ans_scores_test.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute score for answeer training set using user's favorite activities\n",
    "fav_scores_train = []\n",
    "for i in range(len(ans_u_ids_train)):\n",
    "    q = ans_q_ids_train[i]\n",
    "    uid = ans_u_ids_train[i]\n",
    "    I_ua = fav_user_items_train[uid].copy()\n",
    "    Q_tags = ans_item_tags_train\n",
    "    I_tags = fav_item_tags_train\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    fav_scores_train.append(score)\n",
    "\n",
    "#Compute score for answer test set using user's favorite activities\n",
    "fav_scores_test = []\n",
    "for i in range(len(ans_u_ids_test)):\n",
    "    q = ans_q_ids_test[i]\n",
    "    uid = ans_u_ids_test[i]\n",
    "    I_ua = fav_user_items_test[uid].copy()\n",
    "    Q_tags = ans_item_tags_test\n",
    "    I_tags = fav_item_tags_test\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    fav_scores_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute score for answeer training set using user's fork activities\n",
    "fork_scores_train = []\n",
    "for i in range(len(ans_u_ids_train)):\n",
    "    q = ans_q_ids_train[i]\n",
    "    uid = ans_u_ids_train[i]\n",
    "    I_ua = fork_user_items_train[uid].copy()\n",
    "    Q_tags = ans_item_tags_train\n",
    "    I_tags = fork_item_tags_train\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    fork_scores_train.append(score)\n",
    "\n",
    "#Compute score for answer test set using user's fork activities\n",
    "fork_scores_test = []\n",
    "for i in range(len(ans_u_ids_test)):\n",
    "    q = ans_q_ids_test[i]\n",
    "    uid = ans_u_ids_test[i]\n",
    "    I_ua = fork_user_items_test[uid].copy()\n",
    "    Q_tags = ans_item_tags_test\n",
    "    I_tags = fork_item_tags_test\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    fork_scores_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute score for answeer training set using user's watch activities\n",
    "watch_scores_train = []\n",
    "for i in range(len(ans_u_ids_train)):\n",
    "    q = ans_q_ids_train[i]\n",
    "    uid = ans_u_ids_train[i]\n",
    "    I_ua = watch_user_items_train[uid].copy()\n",
    "    Q_tags = ans_item_tags_train\n",
    "    I_tags = watch_item_tags_train\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    watch_scores_train.append(score)\n",
    "\n",
    "#Compute score for answer test set using user's watch activities\n",
    "watch_scores_test = []\n",
    "for i in range(len(ans_u_ids_test)):\n",
    "    q = ans_q_ids_test[i]\n",
    "    uid = ans_u_ids_test[i]\n",
    "    I_ua = watch_user_items_test[uid].copy()\n",
    "    Q_tags = ans_item_tags_test\n",
    "    I_tags = watch_item_tags_test\n",
    "    score = UserActivitySim(q, I_ua, Q_tags, I_tags)\n",
    "    watch_scores_test.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup 1: Using only Answer Similarity Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "x_train_list = []\n",
    "for i in range(len(ans_scores_train)):\n",
    "    x_train_list.append([int(ans_u_ids_train[i]),int(ans_scores_train[i])])\n",
    "x_train = np.array(x_train_list)\n",
    "y_train = ans_q_label_train.copy()\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "#clf = svm.SVC(kernel='linear')\n",
    "clf.fit(x_train_list,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "x_test_list = []\n",
    "for i in range(len(ans_scores_test)):\n",
    "    x_test_list.append([int(ans_u_ids_test[i]),int(ans_scores_test[i])])\n",
    "x_test = np.array(x_test_list)\n",
    "y_test = ans_q_label_test.copy()\n",
    "#pred = clf.predict(x_test)\n",
    "pred = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507200797032\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "user_level_truth = {}\n",
    "user_level_pred = {}\n",
    "for i in range(len(pred)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_pred:\n",
    "        user_level_pred[uid] = [pred[i][0]]\n",
    "    else:\n",
    "        user_level_pred[uid].append(pred[i][1])\n",
    "\n",
    "for i in range(len(ans_q_label_test)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_truth:\n",
    "        user_level_truth[uid] = [ans_q_label_test[i]]\n",
    "    else:\n",
    "        user_level_truth[uid].append(ans_q_label_test[i])\n",
    "\n",
    "result = MAP(user_level_truth,user_level_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiment Setup 2: Using Fork, Watch, Answer and Favorite Similarity Feature\n",
    "- Compute the similarity scores for answer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "x_train_list = []\n",
    "for i in range(len(ans_scores_train)):\n",
    "    x_train_list.append([int(ans_u_ids_train[i]),\n",
    "                         int(ans_scores_train[i]),\n",
    "                         int(fav_scores_train[i]),\n",
    "                         int(fork_scores_train[i]),\n",
    "                         int(watch_scores_train[i])])\n",
    "x_train = np.array(x_train_list)\n",
    "y_train = ans_q_label_train.copy()\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "#clf = svm.SVC(kernel='linear')\n",
    "clf.fit(x_train_list,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "x_test_list = []\n",
    "for i in range(len(ans_scores_test)):\n",
    "    x_test_list.append([int(ans_u_ids_test[i]),\n",
    "                        int(ans_scores_test[i]),\n",
    "                        int(fav_scores_test[i]),\n",
    "                        int(fork_scores_test[i]),\n",
    "                        int(watch_scores_test[i])])\n",
    "x_test = np.array(x_test_list)\n",
    "y_test = ans_q_label_test.copy()\n",
    "#pred = clf.predict(x_test)\n",
    "pred = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442047686048\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "user_level_truth = {}\n",
    "user_level_pred = {}\n",
    "for i in range(len(pred)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_pred:\n",
    "        user_level_pred[uid] = [pred[i][0]]\n",
    "    else:\n",
    "        user_level_pred[uid].append(pred[i][1])\n",
    "\n",
    "for i in range(len(ans_q_label_test)):\n",
    "    uid = ans_u_ids_test[i]\n",
    "    if uid not in user_level_truth:\n",
    "        user_level_truth[uid] = [ans_q_label_test[i]]\n",
    "    else:\n",
    "        user_level_truth[uid].append(ans_q_label_test[i])\n",
    "\n",
    "result = MAP(user_level_truth,user_level_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
